---
title: "第3章 遅延学習"
author-meta: "Sampo Suzuki"
pagetitle: "DAWS2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, fig.align = "center")

htmltools::tagList(rmarkdown::html_dependency_font_awesome())

require(tidymodels)
require(tidyverse)
RNGversion("3.5.3")

seed <- 1540
wbcd_data <- "https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2003/wisc_bc_data.csv"
```

　  
　第三章で学ぶのは **分類を目的** とした「教師あり学習」アルゴリズムである **最近傍法** です。多変量なデータや多水準なデータを分類するのに適しています。機械学習サイトである [scikit-learn <i class="fa fa-external-link"></i>](https://scikit-learn.org/stable/){target="_blank" title="scikit-learn"} のアルゴリズム選択チートシートでは左上の「classification」に属します。  

```{r, echo=FALSE, out.width="80%", fig.cap="scikit-learn algorithm cheat-sheet"}
knitr::include_graphics("https://scikit-learn.org/stable/_static/ml_map.png")
```

　  
　最近傍法の基本的な考え方は

* 同一の水準のデータならば、変量も同質の傾向になる

　という考え方に基づいています。すなわち最近傍法を用いた分類ではトレーニング用データに大きく依存してしまう傾向がありますのでトレーニング用データをポイントになります。  

　  

# $k$ 近傍法とは
　k近傍法（k-nearest neighbor algorithm, k-NN）は、分類したい任意のデータ（以降、インスタンス）の **近傍にある $k$ 個のトレーニング用インスタンスを用いて分類** するアルゴリズムです。つまり分類した任意のインスタンスの近傍にある $k$ 個のトレーニング用インスタンスを用いた多数決による分類です。トレーニング用データに基づく学習の一種で遅延学習または怠惰学習（Lazy Learning）に分類されます。$k$ 近傍法を適用するにはインスタンスとインスタンスの2点間の距離（一般的には **ユークリッド距離（$L_2$ 距離）**）を計算するためのデータならびに多数決のためのラベリングデータが必要です。  
　  
　$k$ 近傍法の特徴は  

**長所**  

* 単純で効果的
* トレーニングデータの分布に前提条件がない
    * 分類境界が不規則な場合に効果を発揮することが多い
* トレーニングが高速

**短所**  

* モデルを作らないので結果の理解が難しい
    * アルゴリズム自体が単純なので説明しやすいという主張もある
* $k$ の値によって結果が変わることがある
* データが多いとトレーニング後の分類処理に時間がかかる
    * 距離を総当たり（Brute Force）で計算するため
    * 高速に計算する方法が色々と研究・実装されている（`FNN` パッケージなど）
* 名義フィーチャー（変数）と欠損値に対する処理が必要

　  
　短所もありますが分類系の問題には概ね適用でき、レコメンド系（おすゝめ、サジェスト）では様々な業種で活用されているようです。アルゴリズムとしての $k$ 近傍法の選択基準は [Choosing the right estimator <i class="fa fa-external-link"></i>](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html){target="_blank" title="scikit-learn algorithm cheat-sheet"} などで紹介されています。  

　  

## 距離による類似性の測定
　実際にはどのような計算を行っているのでしょうか？前述のように $k$ 近傍法はユークリッド距離（$L_2$ 距離）を用いてテスト用インスタンスの近傍にある $k$ 個のトレーニング用インスタンスを探索し多数決によりどのラベルに属すべきかを計算します。ユークリッド距離はピラゴラスの公式（直角三角形の3辺の長さの関係は斜辺を $c$ 、残る2辺を $a, b$ とした場合 $c^2 = a^2 + b^2$ が成り立つ）により求めることができます。  
　$n$ 次元の直交空間における二つのインスタンス（P, Q）間のユークリッド距離（$L_2$ 距離） $d_2(P, Q)$ は  

$$d_2(P, Q) = \sqrt{(P_1 - Q_1)^2 + (P_2 - Q_2)^2 + ... + (P_n - Q_n)^2} $$
$$= \sqrt{\sum_{i=1}^{n}(P_i - Q_i)^2} = d_2(Q, P)$$

　で与えられます。  

　  

### 【参考】距離
　距離にはユークリッド距離（$L_2$ 距離）の他にマンハッタン距離（$L_1$ 距離）と呼ばれる距離があります。その計算方法は以下の通りです。

$$d_1(P, Q) = \sum_{i = 1}^{n}{|P_i - Q_1|} = d_1(Q, P)$$

　ちなみにマンハッタン距離の名前はニューヨーク州のマンハッタン島のような升目状の道路を移動する際の距離に由来しているらしいです。他にも $n$ 次元における距離はマハラノビス距離、チェビシェフ距離、ミンコフスキー距離などがあります。  
　ミンコフスキー距離（$L_n$）はユークリッド距離やマンハッタン距離を一般化した距離で以下の式により定義されます。 $p = 1$ の場合はマンハッタン距離、 $p = 2$ の場合はユークリッド距離になります。

$$d_p(P, Q) = (\sum_{i = 1}^{n}|P_i - Q_i|^p)^{\frac{1}{p}}$$

　  

## 最適な $k$ の選択
　$k$ 近傍法は前述のようにユークリッド距離を用いて近傍の $k$ 個を探索し、探索された $k$ 個のインスタンスを用いて分類しますので、探索する数（$k$）を変えると分類結果が変わる場合があります。$k$ を小さくし過ぎるとゲインが高くなり汎化性能が確保できなくなります。  
　最適な $k$ の値を求めるには交差検証を用いるのが一般的です。交差検証を行うには `e1071` パッケージや `caret` パッケージを用います。具体的な計算方法は後述します。  

　  

# 必要な関連知識
　$k$ 近傍法を利用するにあたり知っておくべき関連知識には以下のようなものがあります。  
　  

* データの正規化
    * 最小最大正規化
    * $Z$ スコア正規化
* ダミーコーディング

　  

## データの正規化
　$k$ 近傍法では各変量を持ちたユークリッド距離を用いて近傍インスタンスを探索しますので、各変量は適切な範囲にある必要があります。例えば、 `x, y, z` という三つの変量をもつ三つのインスタンス間のユークリッド距離を求めてみます。二つのデータセットの違いは変量 `y` が取る値の範囲が一桁異なっている点です。
```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(4, 5, 6), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Sample A")

data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Sample B")
```

　R でユークリッド距離を求めるには `dist` 関数を利用しますが、引数が `matrix` 形式な点に注意してください。
```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(4, 5, 6), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  dist(diag = TRUE) %>% as.matrix() %>% 
  knitr::kable(caption = "Euclidean distance of Sample A")

data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  dist(diag = TRUE) %>% as.matrix() %>% 
  knitr::kable(caption = "Euclidean distance of Sample B")
```

　このように `y` の影響によりユークリッド距離が大きく異なってくることが分かります。変量が取る範囲が異なるとユークリッド距離が大きく代わり探索対象となるインスタンスが変わってしまします。そこで変量が取る範囲による影響を最小限するために正規化という手法を用います。正規化は無次元量化ですから変量の単位に囚われることなくユークリッド距離を計算できるというメリットもあります。　  

　  

### 最小最大正規化
　最小最大正規化（min-max normalization）は変量の取る値が $0$ から $1$ の間になるようにする正規化手法です。 R では既存の関数がありませんので定義する必要があります。

$$x_{new} = \frac{x - min(x)}{max(x) - min(x)} = \frac{x - min(x)}{diff(range(x))}$$
　  
　`normalize` という関数名で最小最大正規化関数を以下のように定義します。なお、引数チェックなどは省略します。  
```{r}
normalize <- function(x) {
  return((x - min(x)) / diff(range(x)))
}
```
　  
　`range` 関数はベクトルの最小値と最大値を返す関数で `diff` 関数は二つの値の差を返す関数です。さきほどの「Sample B」に対して最小最大正規化を適用しユークリッド距離を求めてみます。
```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Sample B")
```

```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  dplyr::mutate_if(is.numeric, .funs = normalize) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Normalized Sample B")
```

```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  dplyr::mutate_if(is.numeric, .funs = normalize) %>% 
  tibble::column_to_rownames() %>% 
  dist(diag = TRUE) %>% as.matrix() %>% 
  knitr::kable(caption = "Euclidean distance of Normalized Sample B")
```

　  

### $Z$ スコア正規化
　$Z$ スコア正規化（Z-score normalization）は平均値が $0$ 、分散が $1$ の標準正規分布にしたがような正規化手法です。 R では `scale` 関数として用意されていますので定義をする必要はありません。ただし、`scale` 関数の返り値は属性付きマトリクス型である点に点に注意してください。  

$$x_{new} = \frac{x - \mu_{x}}{\sigma_x} = \frac{x - mean(x)}{sd(x)} = scale(x)$$

　  
　さきほどの「Sample B」に対して $Z$ スコア正規化を適用しユークリッド距離を求めてみます。
```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Sample B")
```

```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  dplyr::mutate_if(is.numeric, .funs = scale) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Z score Normalized Sample B")
```

```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  dplyr::mutate_if(is.numeric, .funs = scale) %>% 
  tibble::column_to_rownames() %>% 
  dist(diag = TRUE) %>% as.matrix() %>% 
  knitr::kable(caption = "Euclidean distance of Z score Normalized Sample B")
```

　  

## ダミーコーディング
　数値データは最小最大正規化や $Z$ スコア正規化により適切な値の範囲に変換できますが、カテゴリカルデータ（名義尺度）の場合はダミーコーディング（ダミー変数化）により各水準を数値化します。例えば、よくある性別データ（`sex`）はダミーコーディングにより以下のように数値データ（`mail`）に置き換えることができます。  
```{r}
data.frame(sex = c("M", "F", "M", "F", "M", "M", "M", "F")) %>% 
  # 男性（"M"）ならば 1 異なるなら 0 を割り当てる
  dplyr::mutate(male = dplyr::if_else(sex == "M", 1, 0)) %>% 
  knitr::kable(caption = "Dummy Coding")
```

　この場合、数値の大きさに意味はありませんので任意の数値を割り当てることができますが最小最大正規化と同様の値を割り当てることをおすゝめします。  
　  
　続いては三水準のカテゴリカルデータに対するダミーコーディング例です。三水準を二変数の組み合わせで表現している点に着目してください。
```{r}
data.frame(x = c("Hot", "Hot", "Cold", "Hot", "Medium", "Hot", "Cold", "Cold")) %>% 
  dplyr::mutate(hot = dplyr::if_else(x == "Hot", 1, 0),
                cold = dplyr::if_else(x == "Cold", 1, 0))
```

　このようにダミーコーディングを使うと水準数が $n$ のカテゴリ変数は $n - 1$ 個の数値変量（インジケータ）に落とし込むことができます。  
　  
　カテゴリ変数の水準間の大小が決まっており等間隔である場合は以下のように水準を間隔尺度に変換してから正規化処理して直接数値へ落とし込む方法もあります。
```{r}
data.frame(size = c("Large", "Large", "Small", "Large", "Medium",
                    "Large", "Small", "Small")) %>% 
  dplyr::mutate(size_new = dplyr::if_else(size == "Small", 0,
                                          dplyr::if_else(size == "Medium", 1, 2)) %>% 
                  normalize(.))
```

## One-Hot エンコーディング
　ダミーコーディングと似たエンコーディングに One-Hot エンコーディングというものもあります。One-Hot エンコーディングはインスタンスが属す水準に対してフラグ（$1$）を立てるものです。例えば、前述の性別を One-Hot エンコーディングすると以下のようになります。
```{r}
data.frame(sex = c("M", "F", "M", "F", "M", "M", "M", "F")) %>% 
  # 男性（"M"）ならば 1 異なるなら 0 を割り当てる
  dplyr::mutate(male = dplyr::if_else(sex == "M", 1, 0),
                female = dplyr::if_else(sex == "F", 1, 0)) %>% 
  knitr::kable(caption = "One-Hot Encoding")
```
　ダミーコーディングとの一番の違いは $n$ 個の水準が $n$ 個の数値変量になる点です。  

　その他、 Effect コーディングと呼ばれる手法もありますが、ここでは説明を省略しますので、ご自身で調べてみてください。  

　  

# 実例
　『MLwR 2nd』で利用しているサンプルデータのオリジナルは [UC Irvineの機械学習リポジトリ <i class="fa fa-external-link"></i>](http://archive.ics.uci.edu/ml){target="_blank" title="UC Irvine Machine Learning Repository"} にある [ウィスコンシン州のがん検診データ <i class="fa fa-external-link"></i>](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)){target="_blank" title="Breast Cancer Wisconsin (Diagnostic) Data Set"} です。各変量が何を意味するかは『MLwR 2nd』やオリジナルデータのリポジトリで確認してください。

　  

## データの収集
　データは [GitHub <i class="fa fa-external-link"></i>](https://github.com/dataspelunking/MLwR){target="_blank" title=""} にある『MLwR 2nd』のサンプルデータを使用します。  
```{r, echo=FALSE}
wbcd <- read.csv(wbcd_data, stringsAsFactors = FALSE)
wbcd_org <- wbcd
```
　ここではデータをサンプルコードと同様に `wbcd` というデータフレーム型の変数に格納してあります。また、診断結果を表す `diagnosis` 変数もサンプルコードと同様に文字データとして読み込んであります。具体的な読み込み方はサンプルコードで確認してください。
```{r}
wbcd
```

　  

## データの研究と準備
　分析の前に対象データがどのような傾向を持っているのかを把握しておきます。データの要約には `skimr` パッケージが便利です。 `skimr` パッケージがインストールされていない場合は以下のようにパッケージをインストールし読みこんでください。
```{r, eval=FALSE}
install.packages("skimr")
library(skimr)
```

　`skimr` パッケージを使って対象データを要約すると `r length(wbcd)` 個の変量があり、インスタンス数は `r nrow(wbcd)` 個、欠損値（`n_missing`）はないことが確認できます。また、各変量の取る値の範囲は様々ですので正規化処理が必要なことが分かります。
```{r}
skimr::skim(wbcd)
```

　以降の処理では `tidyverse` パッケージを利用しますので読みこんでおきます。
```{r, eval=FALSE}
library(tidyverse)
```

　  

### 前処理
　変量 `id` は受診者の識別番号ですので $k$ 近傍法での距離計算の対象にはなりませんので削除しておきます。
```{r}
wbcd <- wbcd %>% 
  dplyr::select(-id)
wbcd
```

　  
　サンプルコードと同様に診断結果の `diagnosis` の水準名を変更し因子型変数に変換します。因子型変数を扱うには `tidyverse` ファミリーである `forcats` パッケージが便利です。 `forcats` パッケージは `tidyverse` パッケージを読み込んだ際に一緒に読み込まれています。
```{r, include=FALSE, eval=FALSE}
wbcd <- wbcd %>% 
  dplyr::mutate(diagnosis = forcats::fct_inorder(diagnosis)) %>% 
  dplyr::mutate(diagnosis = forcats::fct_recode(diagnosis,
                                                Malignant = "M",
                                                Benign = "B"))
wbcd
```

```{r}
wbcd <- wbcd %>% 
  dplyr::mutate(diagnosis = forcats::fct_inorder(diagnosis)) %>% 
  dplyr::mutate(diagnosis = forcats::fct_recode(diagnosis,
                                                `悪性` = "M",
                                                `良性` = "B"))
wbcd
```

　$2$ 行目の`forcats::fct_inorder` 関数は指定した変量に含まれる水準（`levels`）を出現順に並べ変える関数です。  
　$3$ 行目の `forcats::fct_recode` 関数は水準名を変更する関数です。 `B` は良性、`M` は悪性の意味ですので、分かりやすく置き換えておきます。  
　  
　以上で前処理は終了です。
　  

### 正規化
　次に各変量が取る値の範囲がかなり異なっていますので、全ての数値変量に対して最小最大正規化を適用し、その結果を新たなデータフレーム型変数 `wbcd_n` に格納します。
```{r}
wbcd_n <- wbcd %>%  
  dplyr::mutate_if(is.numeric, normalize)
wbcd_n
```

　  
　念のために正規化後に数値変量がどのような範囲になっているか確認しておきます。
```{r}
wbcd_n %>% 
  dplyr::select_if(is.numeric) %>% 
  skimr::skim()
```

　最小最大正規化後は全ての数値変量が $0$ ～ $1$ の範囲のデータになっていることが分かります。また、各変量は概ね右に歪んだ分布であることもわかります。  

　  

### データの作成
　前処理が終わりましたのでトレーニング用インスタンスとテスト用インスタンスを作成します。『MLwR 2nd』では最初の $469$ インスタンスをトレーニング用、残りの $100$ インスタンスをテスト用として分けています。機械学習では、このようにデータを二分割してトレーニングとテストを行う方法を **ホールドアウト法（またはテストサンプル法）** といいます。  

　  

#### トレーニング用データ
　トレーニング用データとラベルを作成します。
```{r}
wbcd_train <- wbcd_n %>% 
  tibble::rowid_to_column("No") %>%     # フィルタリングするための行番号を変量に
  dplyr::filter(No <= 469) %>%          # 最初の469インスタンスを行番号で抽出
  dplyr::select(-diagnosis) %>%         # ラベル変量を削除
  tibble::column_to_rownames("No")      # 行番号の変量をカラム名に
wbcd_train
```

　トレーニング用ラベルには `diagnosis` 変量を用いますが、コードの都合上データフレーム型変数でなくベクトル変数で保持する必要があります。
```{r}
train_labels <- wbcd_n %>% 
  tibble::rowid_to_column("No") %>% 
  dplyr::filter(No <= 469) %>% 
  dplyr::select(diagnosis) %>% 
  .$diagnosis                           # ラベル用の変量のみを取り出す
train_labels
```

　  

#### テスト用データ
　トレーニング用と同様にテスト用データとラベルを作成します。
```{r}
wbcd_test <- wbcd_n %>% 
  tibble::rowid_to_column("No") %>% 
  dplyr::filter(No > 469) %>%         # フィルタリング条件をひっくり返すだけ
  dplyr::select(-diagnosis) %>% 
  tibble::column_to_rownames("No")
wbcd_test
```

```{r}
test_labels <- wbcd_n %>% 
  tibble::rowid_to_column("No") %>% 
  dplyr::filter(No > 469) %>% 
  dplyr::select(diagnosis) %>% 
  .$diagnosis
test_labels
```

　  

##### ランダムサンプリングによるデータの作成
　『MLwR 2nd』では
　対象データのIDを見る限りランダムサンプリングしたデータのようなのでトレーニングデータ、テスト用を作成するのに更にランダムサンプリングする必要はなさそうですが、ランダムサンプリングをしてトレーニング用、テスト用データを作成する場合には`rsample`パッケージが便利です。
```{r}
# 指定した比率にしたがってランダムに分割データを作るので実行は一回のみなので
# トレーニング用、テスト用に二回実行してはいけない。結果は`.$in_id`に格納される
split <- wbcd_n %>% 
  rsample::initial_split(prop = 1 - 100/569)

split %>% 
  rsample::training()   # .$in_idに該当するデータを取り出す
split %>% 
  rsample::testing()    # .$in_idに該当しないデータを取り出す
```

　  
同様の処理は`dplyr`パッケージだけでも実現可能です。こちらは比率だけでなく個数でも指定可能です。
```{r, message=FALSE}
(train <- wbcd_n %>% 
  dplyr::sample_n(469))      # 比率の場合は`dplyr::sample_frac`関数を用いる

(test <- wbcd_n %>% 
  dplyr::anti_join(train))   # `dplyr::anti_join`でトレーニング用データを除く
```

　  
また、`dplyr`パッケージでは因子（クラス）毎に同数をサンプリングすることも可能です。
```{r}
iris %>% 
  dplyr::group_by(Species) %>% 
  dplyr::sample_n(10)
```

```{r, include=FALSE}
rm(train, test)
```

　  

#### ラベルの作成 【P72】
`class::knn`関数を使う場合、トレーニング用、テスト用のラベルは必ず"atomic vector"でなければなりません。
```{r, eval=FALSE}
# # トレーニング用データのラベル
# wbcd_train_labels <- wbcd_n %>% 
#   tibble::rowid_to_column("No") %>% 
#   dplyr::filter(No <= 469) %>% 
#   dplyr::select(diagnosis)
# 
# # テスト用データのラベル（性能評価する場合にのみラベルが必要）
# wbcd_test_labels <- wbcd_n %>% 
#   tibble::rowid_to_column("No") %>% 
#   dplyr::filter(No > 469) %>% 
#   dplyr::select(diagnosis)
# 
# # ラベルはベクトル変数でなければならない
# wbcd_train_labels <- wbcd_train_labels$diagnosis
# wbcd_test_labels <- wbcd_test_labels$diagnosis
# 
# wbcd_train_labels
# wbcd_test_labels
```

　  

## ステップ3 - データによるモデルの訓練 【P73】
見出しに「モデル」とありますが、遅延学習ではモデルの作成は行われません。トレーニングデータを用いてテストデータのラベル付けを行うだけです。なお、kの値（最近傍の数）にはトレーニング用インスタンス数の平方根に近い整数（奇数）を指定しています。kの値を奇数にする理由は [「"二項分類（二値分類）"の場合に同票数で分類できなくなる問題を避ける」 <i class="fa fa-external-link"></i>](https://ja.wikipedia.org/wiki/K%E8%BF%91%E5%82%8D%E6%B3%95){target="_blank" title="Wikipedia"} （多数決でタイにならないようにする）ためです。
```{r}
wbcd_test_pred <- class::knn(train = wbcd_train, test = wbcd_test,
                             cl = wbcd_train_labels, k = 21)

wbcd_test_pred
```

　  

## ステップ4 - モデルの性能評価 【P74】
分類ができましたので、クロス集計で性能を評価してみましょう。
```{r}
gmodels::CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
                    prop.chisq = FALSE)
```

　  

## ステップ5 - モデルの性能の改善 【P76】

　  

### 変換 - Zスコア標準化 【P76】
汎化性能を確保するためにZスコア正規化を使った場合を確認しておきます。Zスコア正規化には前述のように`scale`関数を用いますが、`scale`関数の返り値は"atomic vector"にならないので、`purrr::map`関数を用いて`scale`関数を適用します。
```{r}
wbcd_nz <- wbcd %>% 
  purrr::map_if(is.numeric, scale) %>% as.data.frame()
  # dplyr::mutate_if(is.numeric, scale, center = FALSE, scale = FALSE)

wbcd_nz %>% 
  dplyr::select_if(is.numeric) %>%
  skimr::skim_to_wide()
```

　  

#### トレーニング用、テスト用データならびにラベルの作成 【P77】
最小最大正規化と同様のトレーニング用、テスト用データを作成します。
```{r}
split <- wbcd_nz %>% 
  rsample::initial_split(prop = 1 - 100/569)

wbcd_train_z <- split %>% 
  rsample::training()   # .$in_idに該当するデータを取り出す
wbcd_test_z <- split %>% 
  rsample::testing()    # .$in_idに該当しないデータを取り出す

wbcd_train_z_labels <- wbcd_train_z$diagnosis
wbcd_test_z_labels <- wbcd_test_z$diagnosis
```

```{r}
wbcd_test_z_pred <- class::knn(train = wbcd_train_z[, -1],
                               test = wbcd_test_z[, -1],
                               cl = wbcd_train_z_labels, k = 21)

gmodels::CrossTable(x = wbcd_test_z_labels, y = wbcd_test_z_pred,
                    prop.chisq = FALSE)
```

　  
<center> > <font size="+2"> **クロス集計はどちらにすべき？** </font></center>
　  

テキストでは縦軸が"Predict"、横軸が"Actual"と指定しており、因子型の順番から良性（陰性）、悪性（陽性）の順になっています。

　               | **予測（良性）** | **予測（悪性）** | 備考
-----------------|--------------|--------------|-----
**実際（良性）** | 真陰性       | 偽陽性（FP） | 陰性
**実際（悪性）** | 偽陰性（FN） | 真陽性       | 陽性

　  
一方、偽陽性、偽陰性を説明するクロス集計表では以下のように縦軸が"Actual"、横軸が"Predict"になるように指定していることが多いようです。
　  

　               | **疾患（あり）** | **疾患（なし）** | 備考
-----------------|--------------|--------------|-----
**検査（陽性）** | 真陽性       | 偽陽性（FP） | 
**検査（陰性）** | 偽陰性（FN） | 真陰性       | 

　  

### kの別の値 【P77】
kの値を変えることで性能を変えることができるかも知れません。最小最大正規化したデータでkの値を変更（k = 1, 3, 5, 7, 9）した場合の結果を見てみましょう。
```{r}
for (k in c(1, 3, 5, 7, 9)) {
  print(paste0("k = ", k))
  set.seed(seed)
  gmodels::CrossTable(x = wbcd_test_labels,
                      y = class::knn(train = wbcd_train, test = wbcd_test,
                                     cl = wbcd_train_labels, k = k),
                      prop.chisq = FALSE)
}
```

k | 偽陰性(B/M) | 偽陽性(M/B) | error rate
-:|------------:|------------:|-----:
1 | 1           | 3           | 0.04
3 | 2           | 1           | 0.03
5 | 2           | 0           | 0.02
7 | 4           | 0           | 0.04
9 | 4           | 0           | 0.04

エラーレート（(偽陰性数 + 偽陽性数)/テスト数 $= \frac{FN+FP}{FN+FP+TN+TP}$）だけを見ると`k = 5`がベストですが、細胞の良性・悪性判断という分類の目的を考えると偽陰性（分類結果が良性であるが実際には悪性であるケース）を極力減らせる方向に振るべきだと考えられますので`k = 1`が最適といえます。ただ、`k = 1`は最小値でありトレーニング用データに対してセンシティブ過ぎる可能性があり、判断が難しいところです。

　  

### 交差検証 【テキスト外】
交差検証（Cross Validation）とは分割したデータの一部でトレーニングを行い残るデータでテストを行いモデルの妥当性を検証（確認）する方法です。ホールドアウト法と異なるのは分割数分だけトレーニングとテストを繰り返し行う（交差させる）点です。そのためホールドアウト法に比べると時間がかかります。主な交差検証としては

* leave-one-out交差検証（一個抜き交差検証）
* k-fold交差検証（k分割交差検証）

があります。一般的に交差検証では正答率（$= \frac{TP + TN}{TP+TN+FP+FN}$）または誤答率（$1 - 正答率$）を指標としてモデルパラメータの最適値を求めます。k近傍法でkの値を選択するのにはこの交差検証を用いる方法が簡単です。しかし、交差検証を行ったからと言って、求められたkの値が分類目的に合っている保証はありません（例：テキストの事例では偽陰性（FN）が増えるのは好ましくないで単に正答率が高いk値では誤診招く可能性があります）。また、ランダムサンプリングを用いていますので全ての関数で結果が一致する保証はありません（本ページではランダム・シードを固定して再現性を確保できるようにしてはあります）。

　  

#### kknnパッケージによる交差検証
`kknn::train.kknn`関数は"leave-one-out"（一個抜き）交差検証が行える関数です。文字通りデータから一個だけインスタンスを抜き出して、残るインスタンスでトレーニングを行い抜いたインスタンスでテストを行いエラーレート（誤答率$= \ \frac{FP+FN}{TP+TN+FP+FN}$ = $1 - 正答率$）で評価します。これをインスタンスの個数分繰り返して最適なkの値を導き出します。したがって、かなりの計算時間を要します。
```{r}
wbcd_train %>% 
  dplyr::mutate(.label = wbcd_train_labels) %>% 
  dplyr::select(.label, dplyr::everything()) %>% 
  kknn::train.kknn(.label ~ ., data = .)
```

なお、`kknn`パッケージには`cv.kknn`関数という"k-fold"（k-分割）交差検証を行う関数もありますが、使い方がよく分からないので割愛します。

　  

#### e1071パッケージによる交差検証
`e1071::tune.knn`関数は"k-fold"（k-分割）交差検証が行える関数です。文字通りデータをk-分割し、一つをテスト用データ、残り（$k - 1$）をトレーニング用データとし、分割数（$k$）だけ繰り返し`kknn::train.kknn`関数と同様にエラーレート（誤答率）が最も少なくなるkの値を最適なkの値として返します。
```{r, message=FALSE}
set.seed(seed)
(knnfit <- e1071::tune.knn(x = wbcd_train, y = wbcd_train_labels, k = 1:30)) %>% 
  summary()

knnfit %>% 
  plot()
```

　  

#### caretパッケージによる交差検証
`caret::train`関数は様々な交差検証を行える関数です。k近傍法の"k-fold"（k-分割）交差検証（デフォルトは10-fold）を行うには以下のように指定します。kの値の選択には`e1071::tune.knn`関数などとは異なり"Accuracy"（正答率$= \ \frac{TP+TN}{TP+TN+FP+FN}$）が使われます。
```{r, message=FALSE}
set.seed(seed)
(knnfit <- caret::train(x = wbcd_train, y = wbcd_train_labels,
             method = "knn", tuneGrid = expand.grid(k = c(1:30)),
             trControl = caret::trainControl(method = "cv")))
```

可視化も簡単に行えます。
```{r}
knnfit %>% 
  ggplot2::ggplot()
```

繰返し交差検証（上記の交差検証を指定回数繰り返して行う検証）を行うことや、fold数を変更することも可能な便利な関数です。
```{r}
set.seed(seed)
caret::train(x = wbcd_train, y = wbcd_train_labels,
             method = "knn", tuneGrid = expand.grid(k = c(1:30)),
             trControl = caret::trainControl(method = "repeatedcv",
                                             number = 5, repeats = 5))
```


　  

# 3.3 まとめ 【P78】
k近傍法は分類結果しか出てこない（モデルを作らない）ので学習とは言い難い点はありますが、トレーニング用データと数行のコードでかなり正確な分類を行ってくれる便利なアルゴリズムであることが分かりました。ただし、分類結果はトレーニング用データに大きく左右される点、kの値の選択方法はこれが正解という唯一無二の解がない点には注意が必要です。また、様々なパッケージで様々な実装が行われていますので、目的や使い勝手にあったパッケージを選択する必要があります。

モデルの評価方法には正答率（$Accuracy = \frac{TP+TN}{TP+TN+FP+FN} = 1 - \frac{FP+FN}{TP+TN+FP+FN}$）の他に適合率（$Precision = \frac{TP}{TP+FP}$）や再現率（$Recall = \frac{TP}{TP+FN}$）といった指標が使われます。モデルの目的にあった評価指標を選択する必要があります。

　  

# Case Study 【テキスト外】
テキスト以外のデータを用いたk近傍法の [ケーススタディをこちら](./casestrudy_knn.html){title="Case Study"} にまとめてあります。

　  

# 参考資料
* [Lazy Learning - Classification Using Nearest Neighbors <i class="fa fa-external-link"></i>](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/06_LazyLearning_kNN.html){target="_blank" title="University of Michigan: Data Science and Predictive Analytics (UMich HS650)"} 
* [Assignment 6: Lazy Learning - Classification Using Nearest Neighbors <i class="fa fa-external-link"></i>](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/06_LazyLearning_kNN_Assignment.html){target="_blank" title="University of Michigan: Data Science and Predictive Analytics (UMich HS650)"} 
* [Python と R の違い (k-NN 法による分類器) <i class="fa fa-external-link"></i>](https://pythondatascience.plavox.info/python%E3%81%A8r%E3%81%AE%E9%81%95%E3%81%84/k-nn%E6%B3%95＊){target="_blank" title="Python でデータサイエンス"} 
* [Variation on “How to plot decision boundary of a k-nearest neighbor classifier from Elements of Statistical Learning?” <i class="fa fa-external-link"></i>](https://stackoverflow.com/questions/31234621/variation-on-how-to-plot-decision-boundary-of-a-k-nearest-neighbor-classifier-f){target="_blank" title="stack overflow"}
* [Chapter 3 Overview of Statistical Learning <i class="fa fa-external-link"></i>](https://dereksonderegger.github.io/578/3-overview-of-statistical-learning.html){target="_blank" title="STA 578 - Statistical Computing Notes"} 
* [K近傍法の特徴について調べてみた <i class="fa fa-external-link"></i>](https://qiita.com/Tokky0425/items/d28021eb1c2a710ec9f9){target="_blank" title="Qiita"} 
* [Choosing the right estimator <i class="fa fa-external-link"></i>](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html){target="_blank" title="scikit-learn algorithm cheat-sheet"} 
* [機械学習 k近傍法 理論編 <i class="fa fa-external-link"></i>](https://dev.classmethod.jp/machine-learning/2017ad_20171218_knn/){target="_blank" title="DevelopersIO"} 
* [機械学習を使って630万件のレビューに基づいたアニメのレコメンド機能を作ってみよう <i class="fa fa-external-link"></i>](https://www.codexa.net/collaborative-filtering-k-nearest-neighbor/){target="_blank" title="codExa"} 
* [kNNを使いこなす！ <i class="fa fa-external-link"></i>](http://univprof.com/archives/16-12-25-9801690.html){target="_blank" title="大学教授のブログ (データ分析相談所)"} 
* [ダブルクロスバリデーション(モデルクロスバリデーション)でテストデータいらず～サンプルが少ないときのモデル検証～ <i class="fa fa-external-link"></i>](https://datachemeng.com/doublecrossvalidation/){target="_blank" title="データ化学工学研究室(金子研究室)＠明治大学"} 
* [回帰モデル・クラス分類モデルを評価・比較するためのモデルの検証 (Model validation) <i class="fa fa-external-link"></i>](https://datachemeng.com/modelvalidation/){target="_blank" title="データ化学工学研究室(金子研究室)＠明治大学"} 
* [1.6. Nearest Neighbors <i class="fa fa-external-link"></i>](https://scikit-learn.org/stable/modules/neighbors.html){target="_blank" title="scikit-learn"} 
* [TuneGrid and TuneLength in Caret <i class="fa fa-external-link"></i>](http://www.rpubs.com/Mentors_Ubiqum/tunegrid_tunelength){target="_blank" title="Rpubs"} 
* [K-Nearest-Neighbor & Tuning By Caret <i class="fa fa-external-link"></i>](http://testblog234wfhb.blogspot.com/2014/06/k-nearest-neighbor-tuning-by-caret.html){target="_blank" title="Tempest"} 
* [交差検証 <i class="fa fa-external-link"></i>](https://ja.wikipedia.org/wiki/%E4%BA%A4%E5%B7%AE%E6%A4%9C%E8%A8%BC){target="_blank" title="Wikipedia"} 
* [【機械学習】モデル評価・指標についてのまとめと実行( w/Titanicデータセット) <i class="fa fa-external-link"></i>](https://qiita.com/kenmatsu4/items/0a862a42ceb178ba7155){target="_blank" title="Qiita"} 


　  

---
<center> [CC BY-NC-SA 4.0 <i class="fa fa-external-link"></i>](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja), Sampo Suzuki [`r format(Sys.time(), format = '%F(%Z)')`] </center>
