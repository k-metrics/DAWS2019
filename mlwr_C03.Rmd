---
title: "第3章 遅延学習"
author-meta: "Sampo Suzuki"
pagetitle: "DAWS2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, fig.align = "center")

htmltools::tagList(rmarkdown::html_dependency_font_awesome())

require(tidymodels)
require(tidyverse)
RNGversion("3.5.3")

seed <- 1540
wbcd_data <- "https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(2nd%20Ed.)/Chapter%2003/wisc_bc_data.csv"
```

　  
　第三章で学ぶのは **分類を目的** とした「教師あり学習」アルゴリズムである **最近傍法** です。多変量なデータや多水準なデータを分類するのに適しています。機械学習サイトである [scikit-learn <i class="fa fa-external-link"></i>](https://scikit-learn.org/stable/){target="_blank" title="scikit-learn"} のアルゴリズム選択チートシートでは左上の「classification」に属します。  

```{r, echo=FALSE, out.width="80%", fig.cap="scikit-learn algorithm cheat-sheet"}
knitr::include_graphics("https://scikit-learn.org/stable/_static/ml_map.png")
```

　  
　最近傍法の基本的な考え方は

* 同一の水準のデータならば、変量も同質の傾向になる

　という考え方に基づいています。すなわち最近傍法を用いた分類ではトレーニング用データに大きく依存してしまう傾向がありますのでトレーニング用データをポイントになります。  

　  

# $k$ 近傍法とは
　k近傍法（k-nearest neighbor algorithm, k-NN）は、分類したい任意のデータ（以降、インスタンス）の **近傍にある $k$ 個のトレーニング用インスタンスを用いて分類** するアルゴリズムです。つまり分類した任意のインスタンスの近傍にある $k$ 個のトレーニング用インスタンスを用いた多数決による分類です。トレーニング用データに基づく学習の一種で遅延学習または怠惰学習（Lazy Learning）に分類されます。$k$ 近傍法を適用するにはインスタンスとインスタンスの2点間の距離（一般的には **ユークリッド距離（$L_2$ 距離）**）を計算するためのデータならびに多数決のためのラベリングデータが必要です。  
　  
　$k$ 近傍法の特徴は  

**長所**  

* 単純で効果的
* トレーニングデータの分布に前提条件がない
    * 分類境界が不規則な場合に効果を発揮することが多い
* トレーニングが高速

**短所**  

* モデルを作らないので結果の理解が難しい
    * アルゴリズム自体が単純なので説明しやすいという主張もある
* $k$ の値によって結果が変わることがある
* データが多いとトレーニング後の分類処理に時間がかかる
    * 距離を総当たり（Brute Force）で計算するため
    * 高速に計算する方法が色々と研究・実装されている（`FNN` パッケージなど）
* 名義フィーチャー（変数）と欠損値に対する処理が必要

　  
　短所もありますが分類系の問題には概ね適用でき、レコメンド系（おすゝめ、サジェスト）では様々な業種で活用されているようです。アルゴリズムとしての $k$ 近傍法の選択基準は [Choosing the right estimator <i class="fa fa-external-link"></i>](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html){target="_blank" title="scikit-learn algorithm cheat-sheet"} などで紹介されています。  

　  

## 距離による類似性の測定
　実際にはどのような計算を行っているのでしょうか？前述のように $k$ 近傍法はユークリッド距離（$L_2$ 距離）を用いてテスト用インスタンスの近傍にある $k$ 個のトレーニング用インスタンスを探索し多数決によりどのラベルに属すべきかを計算します。ユークリッド距離はピラゴラスの公式（直角三角形の3辺の長さの関係は斜辺を $c$ 、残る2辺を $a, b$ とした場合 $c^2 = a^2 + b^2$ が成り立つ）により求めることができます。  
　$n$ 次元の直交空間における二つのインスタンス（P, Q）間のユークリッド距離（$L_2$ 距離） $d_2(P, Q)$ は  

$$d_2(P, Q) = \sqrt{(P_1 - Q_1)^2 + (P_2 - Q_2)^2 + ... + (P_n - Q_n)^2} $$
$$= \sqrt{\sum_{i=1}^{n}(P_i - Q_i)^2} = d_2(Q, P)$$

　で与えられます。  

　  

## 最適な $k$ の選択
　$k$ 近傍法は前述のようにユークリッド距離を用いて近傍の $k$ 個を探索し、探索された $k$ 個のインスタンスを用いて分類しますので、探索する数（$k$）を変えると分類結果が変わる場合があります。$k$ を小さくし過ぎるとゲインが高くなり汎化性能が確保できなくなります。  
　最適な $k$ の値を求めるには交差検証を用いるのが一般的です。交差検証を行うには `e1071` パッケージや `caret` パッケージを用います。具体的な計算方法は後述します。  

　  

# 必要な関連知識
　$k$ 近傍法を利用するにあたり知っておくべき関連知識には以下のようなものがあります。  
　  

* データの正規化
    * 最小最大正規化
    * $Z$ スコア正規化
* ダミーコーディング

　  

## データの正規化
　$k$ 近傍法では各変量を持ちたユークリッド距離を用いて近傍インスタンスを探索しますので、各変量は適切な範囲にある必要があります。例えば、 `x, y, z` という三つの変量をもつ三つのインスタンス間のユークリッド距離を求めてみます。二つのデータセットの違いは変量 `y` が取る値の範囲が一桁異なっている点です。
```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(4, 5, 6), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Sample A")

data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Sample B")
```

　R でユークリッド距離を求めるには `dist` 関数を利用しますが、引数が `matrix` 形式な点に注意してください。
```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(4, 5, 6), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  dist(diag = TRUE) %>% as.matrix() %>% 
  knitr::kable(caption = "Euclidean distance of Sample A")

data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  dist(diag = TRUE) %>% as.matrix() %>% 
  knitr::kable(caption = "Euclidean distance of Sample B")
```

　このように `y` の影響によりユークリッド距離が大きく異なってくることが分かります。変量が取る範囲が異なるとユークリッド距離が大きく代わり探索対象となるインスタンスが変わってしまします。そこで変量が取る範囲による影響を最小限するために正規化という手法を用います。正規化は無次元量化ですから変量の単位に囚われることなくユークリッド距離を計算できるというメリットもあります。　  

　  

### 最小最大正規化
　最小最大正規化（min-max normalization）は変量の取る値が $0$ から $1$ の間になるようにする正規化手法です。 R では既存の関数がありませんので定義する必要があります。

$$x_{new} = \frac{x - min(x)}{max(x) - min(x)} = \frac{x - min(x)}{diff(range(x))}$$
　  
　`normalize` という関数名で最小最大正規化関数を以下のように定義します。なお、引数チェックなどは省略します。  
```{r}
normalize <- function(x) {
  return((x - min(x)) / diff(range(x)))
}
```
　  
　`range` 関数はベクトルの最小値と最大値を返す関数で `diff` 関数は二つの値の差を返す関数です。さきほどの「Sample B」に対して最小最大正規化を適用しユークリッド距離を求めてみます。
```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Sample B")
```

```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  dplyr::mutate_if(is.numeric, .funs = normalize) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Normalized Sample B")
```

```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  dplyr::mutate_if(is.numeric, .funs = normalize) %>% 
  tibble::column_to_rownames() %>% 
  dist(diag = TRUE) %>% as.matrix() %>% 
  knitr::kable(caption = "Euclidean distance of Normalized Sample B")
```

　  

### $Z$ スコア正規化
　$Z$ スコア正規化（Z-score normalization）は平均値が $0$ 、分散が $1$ の標準正規分布にしたがような正規化手法です。 R では `scale` 関数として用意されていますので定義をする必要はありません。ただし、`scale` 関数の返り値は属性付きマトリクス型である点に点に注意してください。  

$$x_{new} = \frac{x - \mu_{x}}{\sigma_x} = \frac{x - mean(x)}{sd(x)} = scale(x)$$

　  
　さきほどの「Sample B」に対して $Z$ スコア正規化を適用しユークリッド距離を求めてみます。
```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Sample B")
```

```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  dplyr::mutate_if(is.numeric, .funs = scale) %>% 
  tibble::column_to_rownames() %>% 
  knitr::kable(caption = "Z score Normalized Sample B")
```

```{r, echo=FALSE}
data.frame(x = c(1, 2, 3), y = c(40, 50, 60), z = c(7, 8, 9),
           rowname = c("A", "B", "C")) %>% 
  dplyr::mutate_if(is.numeric, .funs = scale) %>% 
  tibble::column_to_rownames() %>% 
  dist(diag = TRUE) %>% as.matrix() %>% 
  knitr::kable(caption = "Euclidean distance of Z score Normalized Sample B")
```

　  

## ダミーコーディング
　数値データは最小最大正規化や $Z$ スコア正規化により適切な値の範囲に変換できますが、カテゴリカルデータ（名義尺度）の場合はダミーコーディング（ダミー変数化）により各水準を数値化します。例えば、よくある性別データ（`sex`）はダミーコーディングにより以下のように数値データ（`mail`）に置き換えることができます。  
```{r}
data.frame(sex = c("M", "F", "M", "F", "M", "M", "M", "F")) %>% 
  # 男性（"M"）ならば 1 異なるなら 0 を割り当てる
  dplyr::mutate(male = dplyr::if_else(sex == "M", 1, 0)) %>% 
  knitr::kable(caption = "Dummy Coding")
```

　この場合、数値の大きさに意味はありませんので任意の数値を割り当てることができますが最小最大正規化と同様の値を割り当てることをおすゝめします。  
　  
　続いては三水準のカテゴリカルデータに対するダミーコーディング例です。三水準を二変数の組み合わせで表現している点に着目してください。
```{r}
data.frame(x = c("Hot", "Hot", "Cold", "Hot", "Medium", "Hot", "Cold", "Cold")) %>% 
  dplyr::mutate(hot = dplyr::if_else(x == "Hot", 1, 0),
                cold = dplyr::if_else(x == "Cold", 1, 0))
```

　このようにダミーコーディングを使うと水準数が $n$ のカテゴリ変数は $n - 1$ 個の数値変量（インジケータ）に落とし込むことができます。  
　  
　カテゴリ変数の水準間の大小が決まっており等間隔である場合は以下のように水準を間隔尺度に変換してから正規化処理して直接数値へ落とし込む方法もあります。
```{r}
data.frame(size = c("Large", "Large", "Small", "Large", "Medium",
                    "Large", "Small", "Small")) %>% 
  dplyr::mutate(size_new = dplyr::if_else(size == "Small", 0,
                                          dplyr::if_else(size == "Medium", 1, 2)) %>% 
                  normalize(.))
```

　  

# 実例
　『MLwR 2nd』で利用しているサンプルデータのオリジナルは [UC Irvineの機械学習リポジトリ <i class="fa fa-external-link"></i>](http://archive.ics.uci.edu/ml){target="_blank" title="UC Irvine Machine Learning Repository"} にある [ウィスコンシン州のがん検診データ <i class="fa fa-external-link"></i>](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)){target="_blank" title="Breast Cancer Wisconsin (Diagnostic) Data Set"} です。各変量が何を意味するかは『MLwR 2nd』やオリジナルデータのリポジトリで確認してください。
　  
　以降に掲載しているコードは『MLwR 2nd』のサンプルコードと異なり以下のパッケージを利用しています。

Package      | Version   | Descriptions
-------------|-----------|---------------------------------------------------
class        | `r packageVersion('class')` | Functions for Classification
caret        | `r packageVersion('caret')` | Classification and Regression Training
e1071        | `r packageVersion('e1071')` | Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien
skimr        | `r packageVersion('skimr')` | Compact and Flexible Summaries of Data
tidymodels   | `r packageVersion('tidymodels')` | Easily Install and Load the 'Tidymodels' Packages
tidyverse    | `r packageVersion('tidyverse')` | Easily Install and Load the 'Tidyverse'

　  

## データの収集
　データは [GitHub <i class="fa fa-external-link"></i>](https://github.com/dataspelunking/MLwR){target="_blank" title=""} にある『MLwR 2nd』のサンプルデータを使用します。  
```{r, echo=FALSE}
wbcd <- read.csv(wbcd_data, stringsAsFactors = FALSE)
wbcd_org <- wbcd
```
　ここではデータをサンプルコードと同様に `wbcd` というデータフレーム型の変数に格納してあります。また、診断結果を表す `diagnosis` 変数もサンプルコードと同様に文字データとして読み込んであります。具体的な読み込み方はサンプルコードで確認してください。
```{r}
wbcd
```

　  

## データの研究と準備
　分析の前に対象データがどのような傾向を持っているのかを把握しておきます。`skimr` パッケージを使って対象データを要約すると `r length(wbcd)` 個の変量があり、インスタンス数は `r nrow(wbcd)` 個、欠損値（`n_missing`）はないことが確認できます。また、各変量の取る値の範囲は様々ですので正規化処理が必要なことが分かります。
```{r}
skimr::skim(wbcd)
```

　以降の処理では `tidyverse` パッケージを利用しますので読みこんでおきます。
```{r, eval=FALSE}
library(tidyverse)
```

　  

### 前処理
　変量 `id` は受診者の識別番号ですので $k$ 近傍法での距離計算の対象にはなりませんので削除しておきます。
```{r}
wbcd <- wbcd %>% 
  dplyr::select(-id)
wbcd
```

　  
　サンプルコードと同様に診断結果の `diagnosis` の水準名を変更し因子型変数に変換します。因子型変数を扱うには `tidyverse` ファミリーである `forcats` パッケージが便利です。 `forcats` パッケージは `tidyverse` パッケージを読み込んだ際に一緒に読み込まれています。
```{r}
wbcd <- wbcd %>% 
  dplyr::mutate(diagnosis = forcats::fct_inorder(diagnosis)) %>% 
  dplyr::mutate(diagnosis = forcats::fct_recode(diagnosis,
                                                `悪性` = "M",
                                                `良性` = "B"))
wbcd
```

　$2$ 行目の`forcats::fct_inorder` 関数は指定した変量に含まれる水準（`levels`）を出現順に並べ変える関数です。  
　$3$ 行目の `forcats::fct_recode` 関数は水準名を変更する関数です。 `B` は良性、`M` は悪性の意味ですので、分かりやすく置き換えておきます。  
　  
　以上で前処理は終了です。
　  

### 正規化
　次に各変量が取る値の範囲がかなり異なっていますので、全ての数値変量に対して最小最大正規化を適用し、その結果を新たなデータフレーム型変数 `wbcd_n` に格納します。
```{r}
wbcd_n <- wbcd %>%  
  dplyr::mutate_if(is.numeric, normalize)
wbcd_n
```

　  
　念のために正規化後に数値変量がどのような範囲になっているか確認しておきます。
```{r}
wbcd_n %>% 
  dplyr::select_if(is.numeric) %>% 
  skimr::skim()
```

　最小最大正規化後は全ての数値変量が $0$ ～ $1$ の範囲のデータになっていることが分かります。また、各変量は概ね右に歪んだ分布であることもわかります。  

　  

### データの作成
　前処理が終わりましたのでトレーニング用インスタンスとテスト用インスタンスを作成します。『MLwR 2nd』では最初の $469$ インスタンスをトレーニング用、残りの $100$ インスタンスをテスト用として分けています。機械学習では、このようにデータを二分割してトレーニングとテストを行う方法を **ホールドアウト法（またはテストサンプル法）** といいます。  

　  

#### トレーニング用データ
　トレーニング用データとラベルを作成します。
```{r}
wbcd_train <- wbcd_n %>% 
  tibble::rowid_to_column("No") %>%     # フィルタリングするための行番号を変量に
  dplyr::filter(No <= 469) %>%          # 最初の469インスタンスを行番号で抽出
  dplyr::select(-diagnosis) %>%         # ラベル変量を削除
  tibble::column_to_rownames("No")      # 行番号の変量をカラム名に
wbcd_train
```

　トレーニング用ラベルには `diagnosis` 変量を用いますが、コードの都合上データフレーム型変数でなくベクトル変数で保持する必要があります。
```{r}
train_labels <- wbcd_n %>% 
  tibble::rowid_to_column("No") %>% 
  dplyr::filter(No <= 469) %>% 
  dplyr::select(diagnosis) %>% 
  .$diagnosis                           # ラベル用の変量のみを取り出す
train_labels
```

　  

#### テスト用データ
　トレーニング用と同様にテスト用データとラベルを作成します。
```{r}
wbcd_test <- wbcd_n %>% 
  tibble::rowid_to_column("No") %>% 
  dplyr::filter(No > 469) %>%           # フィルタリング条件をひっくり返す
  dplyr::select(-diagnosis) %>% 
  tibble::column_to_rownames("No")
wbcd_test
```

```{r}
test_labels <- wbcd_n %>% 
  tibble::rowid_to_column("No") %>% 
  dplyr::filter(No > 469) %>% 
  dplyr::select(diagnosis) %>% 
  .$diagnosis
test_labels
```

　  

##### ランダムサンプリングによるデータの作成
　『MLwR 2nd』ではインスタンスの並び順でトレーニング用インスタンスとテスト用インスタンスに分割しているますが、インスタンスの並び順によってはトレーニング用インスタンスが偏る可能性を否定できません。インスタンスを分割する場合には一般的にランダムサンプリングを用います。R でランダムサンプリングを用いてインスタンスを分割する場合、機械学習用パッケージと言える `tidymodels` パッケージを利用します。  
　`tidymodels` パッケージに含まれる `rsample` パッケージの `initial_split` 関数を用います。 `initial_split` 関数は指定した分割比率にしたがってランダムに分割データを作成します。
```{r}
split <- wbcd_n %>% 
  rsample::initial_split(prop = 1 - 100/569)
```
　分割結果はリスト型変数に格納されます。トレーニング用インスタンスに対するインデックスがリスト型変数内の `$in_id` に格納されますので、そのインデックスを用いて `rsample::training` 関数と `rsample::testing` 関数によりトレーニング用インスタンスとテスト用インスタンスを作成します。
```{r}
split %>% 
  rsample::training()                   # .$in_idに該当するデータを取り出す
split %>% 
  rsample::testing()                    # .$in_idに該当しないデータを取り出す
```

　同様の処理は `dplyr` パッケージでも実現可能です。
```{r, message=FALSE}
wbcd_n %>% 
  dplyr::sample_n(469) %>% print() %>%  # トレーニング用インスタンス
  dplyr::anti_join(wbcd_n, .)           # テスト用インスタンス
```

　  

## データによるモデルの訓練
　$k$ 近傍法ではモデル自体の作成は行われるトレーニング用インスタンスを用いてテスト用インスタンスをラベリングします。また、実行の際には近傍数である $k$ を指定する必要があります。ラベリング処理は前述のように近傍 $k$ 個のインスタンスを用いた多数決ですのでタイを避けるために奇数を指定するのが好ましいといわれています。  
　$k$ 近傍法の関数は色々とありますがここでは `class` パッケージの `class::knn` 関数を用います。`class::knn` 関数は引数にトレーニング用インスタンス、テスト用インスタンス、トレーニング用ラベル、近傍数を与える必要があります。
```{r}
class::knn(train = wbcd_train, test = wbcd_test, cl = train_labels, k = 21)
```

　これだけでテスト用インスタンスのラベリング（分類）は完了です。
　  

## モデルの性能評価
　ラベリングができましたので実際の値（テスト用ラベル）との一致度を混同行列（コンフュージョンマトリクス）を用いて確認します。混同行列は `caret::confusionMatrix` 関数を用いて算出します。
```{r}
result <- class::knn(train = wbcd_train, test = wbcd_test, cl = train_labels, k = 21) %>% 
  caret::confusionMatrix(reference = test_labels, positive = "悪性")
result
```
　約 $`r result$overall[1]*100`\%$ の正解率（Accuracy）が得られていることが分かりますが、本来「悪性」であるものを良性と判断（$`r result$table[3]`$ 件）しているためガンを見逃す可能性が高くあまり好ましい結果とはいえません。  

　  

## モデルの性能の改善
　最小最大正規化では約 $`r result$overall[1]*100`\%$ の正解率が得られましたが偽陰性が $`r result$table[3]`$ 件ありましたのでモデル性能の改善ができるか検討してみます。  

　  

### Zスコア正規化
　正規化手法を $Z$ スコア正規化に変更してモデル性能の改善が図れるかを確認してみます。
```{r}
wbcd_nz <- wbcd %>% 
  purrr::map_if(is.numeric, scale) %>% as.data.frame()

wbcd_nz %>% 
  dplyr::select_if(is.numeric) %>%
  skimr::skim()
```

　  
　トレーニング用とテスト用のインスタンスならびにラベルを作成します。
```{r}
split <- wbcd_nz %>% 
  rsample::initial_split(prop = 1 - 100/569)

wbcd_train_z <- split %>% 
  rsample::training()
wbcd_test_z <- split %>% 
  rsample::testing()

train_z_labels <- wbcd_train_z$diagnosis
test_z_labels <- wbcd_test_z$diagnosis
```

```{r}
class::knn(train = wbcd_train_z[, -1], test = wbcd_test_z[, -1],
           cl = train_z_labels, k = 21) %>% 
  caret::confusionMatrix(reference = test_z_labels, positive = "悪性")
```

　残念ながら正解率、偽陰性ともに悪化していますので正規化手法を変更してもモデル性能は改善できなと言えます。  

　  

### 近傍数の変更
　正規化手法を変更してもモデル性能の改善が図れませんでしたので近傍数 $k$ の値を変えてみます。
```{r}
for (k in seq(from = 1, to = 25, by = 2)) {
result <- class::knn(train = wbcd_train, test = wbcd_test, 
                     cl = train_labels, k = k) %>% 
  caret::confusionMatrix(reference = test_labels, positive = "悪性")
  print(paste0("k = ", k, " ER:", 1 - result$overall[1],
               " 偽陰性:", result$table[3], " 偽陽性: ", result$table[2]))
}
```

　ER（エラーレート $= 1 - 正解率$）と偽陰性、偽陽性数を見ると $K = 5$ あたりが良さそうに見えます。$k = 1$ は偽陰性がミニマムなのでベストのように見えますが $k$ 近傍法のアルゴリズムを考えるとオーバーフィッティング（過学習）の可能性があります。  
　また、『MLwR 2nd』ではトレーニング用とテスト用のインスタンスを作成するたいに単純分割を行っていますが、ランダムサンプリングを行うと違う傾向が見えてくるかも知れません。

　  

# まとめ
　$k$ 近傍法は分類結果しか出てこない（モデルを作らない）ので学習とは言い難い点はありますが、トレーニング用インスタンスと数行のコードでかなり正確な分類を行ってくれる便利なアルゴリズムであることが分かりました。ただし、分類結果はトレーニング用インスタンスに大きく左右される点、近傍数 $k$ の選択方法にはこれが正解という解がない点には注意が必要です。また、様々なパッケージで様々な実装が行われていますので、目的や使い勝手にあったパッケージを選択する必要があります。  
　  
　モデルの評価方法には正答率（$Accuracy = \frac{TP+TN}{TP+TN+FP+FN} = 1 - \frac{FP+FN}{TP+TN+FP+FN}$）の他に適合率（$Precision = \frac{TP}{TP+FP}$）や再現率（$Recall = \frac{TP}{TP+FN}$）といった指標が使われます。モデルの目的にあった評価指標を選択する必要があります。

　  

# Appendix
　『MLwR 2nd』では触れられていない関連知識などを紹介します。  

　  

## 距離 
　距離にはユークリッド距離（$L_2$ 距離）の他にマンハッタン距離（$L_1$ 距離）と呼ばれる距離があります。その計算方法は以下の通りです。

$$d_1(P, Q) = \sum_{i = 1}^{n}{|P_i - Q_1|} = d_1(Q, P)$$

　ちなみにマンハッタン距離の名前はニューヨーク州のマンハッタン島のような升目状の道路を移動する際の距離に由来しているらしいです。他にも $n$ 次元における距離はマハラノビス距離、チェビシェフ距離、ミンコフスキー距離などがあります。  
　  
　中でもミンコフスキー距離（$L_n$）はユークリッド距離やマンハッタン距離を一般化した距離で以下の式により定義されます。  

$$d_p(P, Q) = (\sum_{i = 1}^{n}|P_i - Q_i|^p)^{\frac{1}{p}}$$

　$p = 1$ の場合はマンハッタン距離、 $p = 2$ の場合はユークリッド距離になります。  

　  

## One-Hot エンコーディング
　ダミーコーディングと似たエンコーディングに One-Hot エンコーディングという方法もあります。One-Hot エンコーディングはインスタンスが属す水準に対してフラグ（$1$）を立てるものです。例えば、性別を One-Hot エンコーディングするとダミーコーディングとは異なり以下のようになります。
```{r}
data.frame(sex = c("M", "F", "M", "F", "M", "M", "M", "F")) %>% 
  # 男性（"M"）ならば 1 異なるなら 0 を割り当てる
  dplyr::mutate(male = dplyr::if_else(sex == "M", 1, 0),
                female = dplyr::if_else(sex == "F", 1, 0)) %>% 
  knitr::kable(caption = "One-Hot Encoding")
```

　  
　ダミーコーディングとの一番の違いは $n$ 個の水準が $n$ 個の数値変量になる点です。その他、 Effect コーディングと呼ばれる手法もありますが、ここでは説明を省略します。  

　  

## 交差検証
　交差検証（Cross Validation）とは分割したデータの一部でトレーニングを行い残るデータでテストを行いモデルの妥当性を検証（確認）する方法です。ホールドアウト法と異なるのは分割数分だけトレーニングとテストを繰り返し行う（交差させる）点です。そのためホールドアウト法に比べると時間がかかります。主な交差検証としては  

* leave-one-out交差検証（一個抜き交差検証）
* k-fold交差検証（k分割交差検証）

　  
　があります。一般的に交差検証では正解率率（$= \frac{TP + TN}{TP+TN+FP+FN}$）または誤答率（$1 - 正解率$）を指標としてモデルパラメータの最適値を求めます。$k$ 近傍法で近傍数 $k$ を選択するのにはこの交差検証を用いる方法が簡単です。しかし、交差検証を行ったからと言って、求められた $k$ の値が分類目的に合っている保証はありません（例：テキストの事例では偽陰性（FN）が増えるのは好ましくないで単に正答率が高いk値では誤診招く可能性があります）。また、ランダムサンプリングを用いていますので全ての関数で結果が一致する保証はありません。  
　  
 $k$ 近傍法の交差検証を行えるパッケージとしては以下のようなパッケージがあります。
 
Package      | Descriptions
-------------|------------------------------------------------------------------
caret        | Classification and Regression Training
e1071        | Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien
kknn         | Weighted k-Nearest Neighbors

　ここでは `caret` パッケージによる交差検証を行ってみます。

　  

### caretパッケージによる交差検証
　`caret::train` 関数は様々な交差検証を行える関数です。$k$ 近傍法の "k-fold"（k-分割）交差検証（デフォルトは 10-fold）を行うには以下のように指定します。$k$ 値の選択基準としてはデフォルトで "Accuracy"（正解率$= \ \frac{TP+TN}{TP+TN+FP+FN}$）が使われます。
```{r, message=FALSE}
knnfit <- caret::train(x = wbcd_train, y = train_labels,
                       method = "knn", tuneGrid = expand.grid(k = c(1:30)),
                       trControl = caret::trainControl(method = "cv"))
knnfit
```

　`ggplot2` パッケージによる可視化も簡単に行えます。
```{r}
knnfit %>% 
  ggplot2::ggplot()
```

　繰返し交差検証（上記の交差検証を指定回数繰り返して行う検証）を行うことや、fold数を変更することも可能です。

　  

## 参考資料
* [Lazy Learning - Classification Using Nearest Neighbors <i class="fa fa-external-link"></i>](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/06_LazyLearning_kNN.html){target="_blank" title="University of Michigan: Data Science and Predictive Analytics (UMich HS650)"} 
* [Assignment 6: Lazy Learning - Classification Using Nearest Neighbors <i class="fa fa-external-link"></i>](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/06_LazyLearning_kNN_Assignment.html){target="_blank" title="University of Michigan: Data Science and Predictive Analytics (UMich HS650)"} 
* [Python と R の違い (k-NN 法による分類器) <i class="fa fa-external-link"></i>](https://pythondatascience.plavox.info/python%E3%81%A8r%E3%81%AE%E9%81%95%E3%81%84/k-nn%E6%B3%95＊){target="_blank" title="Python でデータサイエンス"} 
* [Variation on “How to plot decision boundary of a k-nearest neighbor classifier from Elements of Statistical Learning?” <i class="fa fa-external-link"></i>](https://stackoverflow.com/questions/31234621/variation-on-how-to-plot-decision-boundary-of-a-k-nearest-neighbor-classifier-f){target="_blank" title="stack overflow"}
* [Chapter 3 Overview of Statistical Learning <i class="fa fa-external-link"></i>](https://dereksonderegger.github.io/578/3-overview-of-statistical-learning.html){target="_blank" title="STA 578 - Statistical Computing Notes"} 
* [K近傍法の特徴について調べてみた <i class="fa fa-external-link"></i>](https://qiita.com/Tokky0425/items/d28021eb1c2a710ec9f9){target="_blank" title="Qiita"} 
* [Choosing the right estimator <i class="fa fa-external-link"></i>](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html){target="_blank" title="scikit-learn algorithm cheat-sheet"} 
* [機械学習 k近傍法 理論編 <i class="fa fa-external-link"></i>](https://dev.classmethod.jp/machine-learning/2017ad_20171218_knn/){target="_blank" title="DevelopersIO"} 
* [機械学習を使って630万件のレビューに基づいたアニメのレコメンド機能を作ってみよう <i class="fa fa-external-link"></i>](https://www.codexa.net/collaborative-filtering-k-nearest-neighbor/){target="_blank" title="codExa"} 
* [kNNを使いこなす！ <i class="fa fa-external-link"></i>](http://univprof.com/archives/16-12-25-9801690.html){target="_blank" title="大学教授のブログ (データ分析相談所)"} 
* [ダブルクロスバリデーション(モデルクロスバリデーション)でテストデータいらず～サンプルが少ないときのモデル検証～ <i class="fa fa-external-link"></i>](https://datachemeng.com/doublecrossvalidation/){target="_blank" title="データ化学工学研究室(金子研究室)＠明治大学"} 
* [回帰モデル・クラス分類モデルを評価・比較するためのモデルの検証 (Model validation) <i class="fa fa-external-link"></i>](https://datachemeng.com/modelvalidation/){target="_blank" title="データ化学工学研究室(金子研究室)＠明治大学"} 
* [1.6. Nearest Neighbors <i class="fa fa-external-link"></i>](https://scikit-learn.org/stable/modules/neighbors.html){target="_blank" title="scikit-learn"} 
* [TuneGrid and TuneLength in Caret <i class="fa fa-external-link"></i>](http://www.rpubs.com/Mentors_Ubiqum/tunegrid_tunelength){target="_blank" title="Rpubs"} 
* [K-Nearest-Neighbor & Tuning By Caret <i class="fa fa-external-link"></i>](http://testblog234wfhb.blogspot.com/2014/06/k-nearest-neighbor-tuning-by-caret.html){target="_blank" title="Tempest"} 
* [交差検証 <i class="fa fa-external-link"></i>](https://ja.wikipedia.org/wiki/%E4%BA%A4%E5%B7%AE%E6%A4%9C%E8%A8%BC){target="_blank" title="Wikipedia"} 
* [【機械学習】モデル評価・指標についてのまとめと実行( w/Titanicデータセット) <i class="fa fa-external-link"></i>](https://qiita.com/kenmatsu4/items/0a862a42ceb178ba7155){target="_blank" title="Qiita"} 



---
<center> [CC BY-NC-SA 4.0 <i class="fa fa-external-link"></i>](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja), Sampo Suzuki [`r format(Sys.time(), format = '%F(%Z)')`] </center>
